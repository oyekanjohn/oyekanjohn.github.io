<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v5.9.13, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v5.9.13, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/mbr-96x96.png" type="image/x-icon">
  <meta name="description" content="">
  
  
  <title>Publications</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons2/mobirise2.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/animatecss/animate.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Inter+Tight:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter+Tight:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap"></noscript>
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css?v=LLYOnZ"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css?v=LLYOnZ" type="text/css">

  
  
  
</head>
<body>
  
  <section data-bs-version="5.1" class="menu menu2 cid-u3m2GvSSuo" once="menu" id="menu02-2u">
	

	<nav class="navbar navbar-dropdown navbar-fixed-top navbar-expand-lg">
		<div class="container">
			<div class="navbar-brand">
				<span class="navbar-logo">
					<a href="https://mobirise.com">
						<img src="assets/images/mbr-96x96.png" alt="Mobirise Website Builder" style="height: 3rem;">
					</a>
				</span>
				<span class="navbar-caption-wrap"><a class="navbar-caption text-black text-primary display-4" href="index.html#header05-1">FlexiAutoLab</a></span>
			</div>
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbarSupportedContent" data-bs-target="#navbarSupportedContent" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
				<div class="hamburger">
					<span></span>
					<span></span>
					<span></span>
					<span></span>
				</div>
			</button>
			<div class="collapse navbar-collapse" id="navbarSupportedContent">
				<ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item">
						<a class="nav-link link text-black text-primary display-4" href="People.html">People</a>
					</li><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="page6.html">Research</a></li><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="Projects.html">Projects</a></li>
					<li class="nav-item">
						<a class="nav-link link text-black text-primary display-4" href="Publications.html" aria-expanded="false">Publications</a>
					</li>
					<li class="nav-item">
						<a class="nav-link link text-black text-primary display-4" href="outreach.html">Outreach &amp; Media</a>
					</li><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="outreach.html">Open Source</a></li></ul>
				
				
			</div>
		</div>
	</nav>
</section>

<section data-bs-version="5.1" class="header5 cid-u3m2GwoQTR" data-bg-video="https://www.youtube.com/watch?v=bkmari46q-s" id="header05-2v">
	

	<div class="mbr-fallback-image disabled"></div>
	<div class="mbr-overlay" style="opacity: 1; background-color: rgb(12, 44, 104);"></div>
	<div class="topbg"></div>
	<div class="align-center container">
		<div class="row justify-content-center">
			<div class="col-md-12 col-lg-9">
				<h1 class="mbr-section-title mbr-fonts-style mb-4 display-1"><strong>Publications</strong></h1>
				
				
			</div>
		</div>
		<div class="row mt-5 justify-content-center">
			
		</div>
	</div>
</section>

<section data-bs-version="5.1" class="start article05 cid-u3mkTxVnWb" id="article05-3m">
  

  
  

  <div class="container-fluid">
    <div class="row justify-content-center align-items-center">
      <div class="col-12">
        <div class="card-wrapper">
          <div class="row">
            <div class="col-12 col-md-12 col-lg-5 image-wrapper">
              <img class="w-100" src="assets/images/book-659x1000.jpg" alt="Mobirise Website Builder">
            </div>
            <div class="col-12 col-lg col-md-12">
              <div class="text-wrapper align-left">
                <h1 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                  <strong>Our book on bio-inspired intelligence for multi-agent systems</strong></h1>
                <p class="mbr-text mbr-fonts-style mb-4 display-7">
                  You can buy a copy on Amazon<br>
                </p>
                <div class="mbr-section-btn mt-3"><a class="btn btn-lg btn-black display-7" href="https://www.amazon.co.uk/Tracking-Spatiotemporal-Quantities-Unicellular-Intelligence/dp/3319801392">Amazon</a></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section data-bs-version="5.1" class="article07 cid-u44rhrkKfi" id="article07-4g">
  

  
  
  <div class="container-fluid">
    <div class="row justify-content-center">
      <div class="card col-md-12 col-lg-12">
        <div class="card-wrapper">
          <h3 class="card-title mbr-fonts-style mbr-white mt-3 mb-4 display-2"><strong>Key publications:</strong>
          </h3>
          <div class="row card-box align-left">
            <div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://eprints.whiterose.ac.uk/175562/3/Journal_paper_AAIE_Accepted.pdf" class="text-primary"><strong>Application of Attention Deep learning Networks and Acoustics for Machine State Estimation</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">The rapid development of technology is leading to the emergence of smart factories where the Artificial Intelligence paradigm of deep learning plays a significant role in processing data streams from machines. This paper presents the application of Augmented Attention Blocks embedded in a deep convolutional neural network for the purposes of estimating the state of remote machines using remotely collected acoustic data. An Android application was developed for the purposes of transferring audio data from a remote machine to a base station. At the base station, we propose and developed a deep convolutional neural network called MAABL (MobileNetv2 with Augmented Attention Block). The structure of the neural network is constructed by combining an inverted residual block of MobileNetv2 with an augmented attention mechanism block.
                </p>
              </div>
            </div>
            <div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://eprints.whiterose.ac.uk/135604/1/The%20effectiveness%20of%20virtual%20environments%20in%20developing%20collaborative%20strategies%20between%20industrial%20robots%20and%20humans_RCIM_Journal_2018.pdf" class="text-primary"><strong>Application of Virtual Reality and Robotics for studying human robot collaboration strategies</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">Testing and implementation of Human-Robot Collaboration (HRC) could be dangerous due&nbsp;to the high-speed movements and massive forces generated by industrial robots. Wherever humans and industrial robots share a common workplace, accidents are likely to happen and&nbsp;always unpredictable. This has hindered the development of human robot collaborative&nbsp;strategies as well as the ability of authorities to pass regulations on how humans and robots&nbsp;should work together in close proximities. This paper presents the use of a Virtual Reality&nbsp;digital twin of a physical layout as a mechanism to understand human reactions to both&nbsp;predictable and unpredictable robot motions. A set of established metrics as well as a newly&nbsp;developed Kinetic Energy Ratio metric are used to analyse human reactions and validate the&nbsp;effectiveness of the Virtual Reality environment. It is the aim that Virtual Reality digital twins&nbsp;could inform the safe implementation of Human-Robot Collaborative strategies in factories of
<br>the future.</p>
              </div>
            </div>
            <div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://eprints.whiterose.ac.uk/179722/3/Wearable%20Sensors%20and%20Cognitive%20Architecture%20%28final%29%20%288%29.pdf" class="text-primary"><strong>Application of Cognitive Architectures, Wearable Sensors and Digital Twins for Real Time Ergonomics Analysis</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">High value manufacturing systems still require ergonomically intensive manual activities.&nbsp;Examples include the aerospace industry where the fitting of pipes and wiring into confined spaces in aircraft wings is still a manual operation. In these environments, workers are&nbsp;subjected to ergonomically awkward forces and postures for long periods of time. This leads&nbsp;to musculoskeletal injuries that severely limit the output of a shopfloor leading to loss of&nbsp;productivity. The use of tools such as wearable sensors could provide a way to track the&nbsp;ergonomics of workers in real time. However, an information processing architecture is&nbsp;required in order to ensure that data is processed in real time and in a manner that meaningful&nbsp;action points are retrieved for use by workers.&nbsp;In this work, based on the Adaptive Control of Thought—Rational (ACT-R) cognitive&nbsp;framework, we propose a Cognitive Architecture for Wearable Sensors (CAWES); a wearable&nbsp;sensor system and cognitive architecture that is capable of taking data streams from multiple&nbsp;wearable sensors on a worker’s body and fusing them to enable digitisation, tracking and&nbsp;analysis of human ergonomics in real time on a shopfloor. Furthermore, through tactile&nbsp;feedback, the architecture is able to inform workers in real time when ergonomics rules are&nbsp;broken. The architecture is validated through the use of an aerospace case study undertaken in&nbsp;laboratory conditions. The results from the validation are encouraging and in the future, further
<br>tests will be performed in an actual working environment.&nbsp;</p>
              </div>
            </div>
            <div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://eprints.whiterose.ac.uk/176759/7/Combining_virtual_reality_enabled_simulation-2017.pdf" class="text-primary"><strong>Application of vision based sensors for Digital Assistive Assembly</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">Recent introduction of low-cost 3D sensing and affordable immersive virtual reality have lowered the&nbsp;barriers for creating and maintaining 3D virtual worlds. In this paper, we propose a way to combine these&nbsp;technologies with discrete-event simulation to improve the use of simulation in decision making in&nbsp;manufacturing. This work will describe how feedback is possible from real world systems directly into a&nbsp;simulation model to guide smart behaviors. Technologies included in the research include feedback from&nbsp;RGBD images of shop floor motion and human interaction within full immersive virtual reality that&nbsp;includes the latest headset technologies.&nbsp;&nbsp;</p>
              </div>
            </div><div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://www.sciencedirect.com/science/article/pii/S0736584523000157" class="text-primary"><strong>Application of Deep Reinforcement Learning and Collaborative Robots for Flexible packaging of goods</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">
                  Current advances in Task and Motion Planning (TAMP) framework often rely on a specific and static task structure. A task structure is a sequence of how work pieces should be manipulated towards achieving a goal. Such systems can be problematic when task structures change as a result of human performance during human-robot collaboration scenarios in manufacturing or when redundant objects are present in the workspace, for example, during a Package-To- Order scenario with the same object type fulfilling different package configurations. In this paper, we propose a novel integrated TAMP framework that supports learning from human demonstrations while tackling variations in object positions and product configurations during massive-Package-To-Order (mPTO) scenarios in manufacturing as well as during human-robot collaboration scenarios. We design and apply a Graph Neural Network(GNN) based high-level reasoning module that is capable of handling variant goal configurations and can generalize to different task structures. Moreover, we also built a two-level motion module which can produce flexible and collision-free trajectories based on important features and task labels produced by the reasoning module. Through simulations and physical experiments, we show that our framework holds several advantages when compared with state-of-the-art previous work. The advantages include sample-efficiency and generalizability to unseen goal configurations as well as task structures.
                </p>
              </div>
            </div><div class="item features-without-image col-12">
              <div class="item-wrapper">
                <h4 class="mbr-section-subtitle mbr-fonts-style mb-3 display-5">
                  <a href="https://www.sciencedirect.com/science/article/pii/S0921889023001288" class="text-primary"><strong>Application of Deep Reinforcement Learning and Nature inspired Algorithms for Collective Load Transport on Unmanned Aerial Vehicles</strong></a></h4>
                <p class="mbr-text mbr-fonts-style display-7">
                  Deep reinforcement learning, by taking advantage of neural networks, has made great strides in the continuous control of robots. However, in scenarios where multiple robots are required to collaborate with each other to accomplish a task, it is still challenging to build an efficient and scalable multi-agent control system due to increasing complexity. In this paper, we regard each unmanned aerial vehicle (UAV) with its manipulator as one agent, and leverage the power of multi-agent deep deterministic policy gradient (MADDPG) for the cooperative navigation and manipulation of a load. We propose solutions for addressing navigation to grasping point problem in targeted and flexible scenarios, and mainly focus on how to develop model-free policies for the UAVs without relying on a trajectory planner. To overcome the challenges of learning in scenarios with an increasing number of grasping points, we incorporate the demonstrations from an Optimal Reciprocal Collision Avoidance (ORCA) algorithm into our framework to guide the policy training and adapt two novel techniques into the architecture of MADDPG.
                </p>
              </div>
            </div>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section data-bs-version="5.1" class="article11 cid-u3mmAstJVb" id="article11-3p">
    

    
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="title col-md-12 col-lg-8">
                <h3 class="mbr-section-title mbr-fonts-style align-center mt-0 mb-0 display-7">
                <strong>The rest of our publications can be found <a href="https://scholar.google.co.uk/citations?user=W7ePGWYAAAAJ&hl=en" class="text-primary">here</a></strong></h3>
                
                
            </div>
        </div>
    </div>
</section><section class="display-7" style="padding: 0;align-items: center;justify-content: center;flex-wrap: wrap;    align-content: center;display: flex;position: relative;height: 4rem;"><a href="https://mobiri.se/3140615" style="flex: 1 1;height: 4rem;position: absolute;width: 100%;z-index: 1;"><img alt="" style="height: 4rem;" src="data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw=="></a><p style="margin: 0;text-align: center;" class="display-7">&#8204;</p><a style="z-index:1" href="https://mobirise.com/builder/landing-page-software.html">Landing Page Software</a></section><script src="assets/bootstrap/js/bootstrap.bundle.min.js"></script>  <script src="assets/smoothscroll/smooth-scroll.js"></script>  <script src="assets/ytplayer/index.js"></script>  <script src="assets/dropdown/js/navbar-dropdown.js"></script>  <script src="assets/vimeoplayer/player.js"></script>  <script src="assets/theme/js/script.js"></script>  
  
  
  <input name="animation" type="hidden">
  </body>
</html>